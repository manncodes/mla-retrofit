{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA-Retrofit Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use MLA-Retrofit to convert a model from GQA to MLA and test the results.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/manncodes/mla-retrofit/blob/main/examples/mla_retrofit_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the MLA-Retrofit package and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLA-Retrofit\n",
    "!pip install git+https://github.com/manncodes/mla-retrofit.git\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes>=0.40.0  # For quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from mla_retrofit import convert_to_mla\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"mla-demo\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model and Parameters\n",
    "\n",
    "Let's set up the parameters for conversion. We'll use a small model for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll use a smaller model that can fit in Colab's memory\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small model for demo\n",
    "output_dir = \"./tinyllama-mla\"\n",
    "\n",
    "# MLA parameters\n",
    "num_kv_heads = 2  # Number of KV heads for MLA\n",
    "head_dim = 64     # Head dimension for MLA\n",
    "rope_mode = \"extend\"  # Mode for RoPE handling\n",
    "absorb = True     # Whether to absorb projection matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine Original Model\n",
    "\n",
    "Before conversion, let's load the original model and examine its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original model\n",
    "logger.info(f\"Loading original model: {model_name}\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Print model configuration\n",
    "print(\"\\nOriginal Model Configuration:\")\n",
    "print(f\"Hidden size: {original_model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {original_model.config.num_attention_heads}\")\n",
    "print(f\"Number of KV heads: {getattr(original_model.config, 'num_key_value_heads', original_model.config.num_attention_heads)}\")\n",
    "print(f\"Head dimension: {original_model.config.hidden_size // original_model.config.num_attention_heads}\")\n",
    "\n",
    "# Calculate memory requirements\n",
    "seq_len = 1024\n",
    "kv_heads = getattr(original_model.config, 'num_key_value_heads', original_model.config.num_attention_heads)\n",
    "head_dim_orig = original_model.config.hidden_size // original_model.config.num_attention_heads\n",
    "kv_cache_size_original = 2 * seq_len * kv_heads * head_dim_orig * 2  # 2 for K and V, 2 bytes for float16\n",
    "print(f\"\\nKV cache size for {seq_len} tokens (FP16): {kv_cache_size_original / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Original Model\n",
    "\n",
    "Let's generate some text with the original model to compare it with the MLA version later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prompt\n",
    "prompt = \"Explain the advantages of Multi-head Latent Attention in language models in simple terms.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with original model\n",
    "logger.info(\"Generating text with original model...\")\n",
    "with torch.no_grad():\n",
    "    outputs_original = original_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text_original = tokenizer.decode(outputs_original[0], skip_special_tokens=True)\n",
    "print(f\"\\nOriginal Model Output:\\n{generated_text_original}\")\n",
    "\n",
    "# Free up memory\n",
    "del original_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to MLA\n",
    "\n",
    "Now, let's convert the model to use Multi-head Latent Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to MLA\n",
    "logger.info(f\"Converting {model_name} to MLA...\")\n",
    "model, tokenizer = convert_to_mla(\n",
    "    model_name_or_path=model_name,\n",
    "    num_kv_heads=num_kv_heads,\n",
    "    head_dim=head_dim,\n",
    "    rope_mode=rope_mode,\n",
    "    absorb=absorb,\n",
    "    flash_attn=False,  # Set to True if you have Flash Attention installed\n",
    "    return_model=True,\n",
    ")\n",
    "\n",
    "# Print MLA model configuration\n",
    "print(\"\\nMLA Model Configuration:\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"Number of KV heads: {model.config.num_key_value_heads}\")\n",
    "print(f\"Head dimension: {model.config.head_dim}\")\n",
    "\n",
    "# Calculate memory requirements\n",
    "kv_cache_size_mla = 2 * seq_len * num_kv_heads * head_dim * 2  # 2 for K and V, 2 bytes for float16\n",
    "print(f\"\\nKV cache size for {seq_len} tokens (FP16): {kv_cache_size_mla / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - kv_cache_size_mla / kv_cache_size_original) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with MLA Model\n",
    "\n",
    "Now let's generate text with the MLA-converted model and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with MLA model\n",
    "logger.info(\"Generating text with MLA model...\")\n",
    "with torch.no_grad():\n",
    "    outputs_mla = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text_mla = tokenizer.decode(outputs_mla[0], skip_special_tokens=True)\n",
    "print(f\"\\nMLA Model Output:\\n{generated_text_mla}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Converted Model\n",
    "\n",
    "Let's save the MLA-converted model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "logger.info(f\"Saving MLA model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Longer Context\n",
    "\n",
    "One benefit of MLA is its ability to handle longer contexts with the same memory. Let's test this with a longer prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a longer prompt (repeated text for demo purposes)\n",
    "long_prompt = prompt + \"\\n\\n\" + \"\\n\\n\".join([f\"Section {i+1}: \" + prompt for i in range(10)])\n",
    "print(f\"Prompt length: {len(tokenizer.encode(long_prompt))} tokens\")\n",
    "\n",
    "# Tokenize long prompt\n",
    "long_inputs = tokenizer(long_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with MLA model on longer context\n",
    "logger.info(\"Generating text with MLA model on longer context...\")\n",
    "with torch.no_grad():\n",
    "    outputs_long = model.generate(\n",
    "        **long_inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text_long = tokenizer.decode(outputs_long[0], skip_special_tokens=True)\n",
    "# Just print the generated part (not the full prompt)\n",
    "print(f\"\\nMLA Model Output on Longer Context:\\n{generated_text_long[len(long_prompt):]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profile and Benchmark\n",
    "\n",
    "Now let's profile the memory usage and generation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory profiling\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCUDA Memory Stats:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved() / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # Simple generation speed benchmark\n",
    "    import time\n",
    "    \n",
    "    # Warm-up\n",
    "    with torch.no_grad():\n",
    "        model.generate(**inputs, max_new_tokens=10)\n",
    "    \n",
    "    # Benchmark\n",
    "    num_runs = 5\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    print(f\"\\nBenchmarking generation speed (average of {num_runs} runs):\")\n",
    "    for i in range(num_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "        run_time = end_time - start_time\n",
    "        total_time += run_time\n",
    "        total_tokens += tokens_generated\n",
    "        \n",
    "        print(f\"Run {i+1}: Generated {tokens_generated} tokens in {run_time:.4f}s ({tokens_generated/run_time:.2f} tokens/s)\")\n",
    "    \n",
    "    print(f\"\\nAverage: {total_tokens/total_time:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the MLA Model (Optional)\n",
    "\n",
    "For even better performance, you may want to fine-tune the converted model. Here's a simplified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell is for demonstration only and won't run well in Colab without additional setup\n",
    "# Uncomment and run if you have sufficient GPU memory and want to try fine-tuning\n",
    "\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load a small dataset for fine-tuning (using alpaca as an example)\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:100]\")  # Just use 100 examples for demo\n",
    "\n",
    "# Format the dataset\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"### Instruction: {example['instruction']}\\n\\n### Input: {example['input']}\\n\\n### Response: {example['output']}\"\n",
    "    }\n",
    "\n",
    "# Apply formatting and tokenization\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"input\", \"output\", \"text\"],\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-mla-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_steps=10,  # Just a few steps for the demo\n",
    "    logging_steps=1,\n",
    "    save_steps=5,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./tinyllama-mla-finetuned\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Convert a model from standard attention or GQA to MLA\n",
    "2. Compare the memory usage before and after conversion\n",
    "3. Test the model on both short and longer context prompts\n",
    "4. Benchmark generation speed\n",
    "5. (Optionally) Fine-tune the converted model\n",
    "\n",
    "MLA-Retrofit provides a simple way to enhance existing models by adding the benefits of Multi-head Latent Attention without requiring full retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
