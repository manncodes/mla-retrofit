{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA-Retrofit Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use MLA-Retrofit to convert a model from GQA to MLA and test the results.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/manncodes/mla-retrofit/blob/main/examples/mla_retrofit_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the MLA-Retrofit package and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLA-Retrofit\n",
    "!pip install git+https://github.com/manncodes/mla-retrofit.git\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes>=0.40.0  # For quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\sandbox\\repo\\mla-retrofit\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# get curr working dir\n",
    "cwd = pathlib.Path.cwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 17:01:11,188 - mla-demo - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from mla_retrofit import convert_to_mla\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"mla-demo\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model and Parameters\n",
    "\n",
    "Let's set up the parameters for conversion. We'll use a small model for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll use a smaller model that can fit in Colab's memory\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small model for demo\n",
    "output_dir = \"./tinyllama-mla\"\n",
    "\n",
    "# MLA parameters\n",
    "num_kv_heads = 4  # Number of KV heads for MLA\n",
    "head_dim = 64     # Head dimension for MLA\n",
    "rope_mode = \"extend\"  # Mode for RoPE handling\n",
    "absorb = True     # Whether to absorb projection matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine Original Model\n",
    "\n",
    "Before conversion, let's load the original model and examine its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 17:13:14,225 - mla-demo - INFO - Loading original model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-03-23 17:13:14,555 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Model Configuration:\n",
      "Hidden size: 2048\n",
      "Number of attention heads: 32\n",
      "Number of KV heads: 4\n",
      "Head dimension: 64\n",
      "KV cache size: 2 * 2 * 1024(seq_len) * 4(kv_heads) * 64(head_dim) * 22(n_attn_layers) = 22.00 MB\n",
      "\n",
      "KV cache size for 1024 tokens (FP16): 22.00 MB\n",
      "per token KV cache size: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "# Load original model\n",
    "logger.info(f\"Loading original model: {model_name}\")\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Print model configuration\n",
    "print(\"\\nOriginal Model Configuration:\")\n",
    "print(f\"Hidden size: {original_model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {original_model.config.num_attention_heads}\")\n",
    "print(f\"Number of KV heads: {getattr(original_model.config, 'num_key_value_heads', original_model.config.num_attention_heads)}\")\n",
    "print(f\"Head dimension: {original_model.config.hidden_size // original_model.config.num_attention_heads}\")\n",
    "\n",
    "# Calculate memory requirements\n",
    "seq_len = 1024\n",
    "kv_heads = getattr(original_model.config, 'num_key_value_heads', original_model.config.num_attention_heads)\n",
    "n_attn_layers = original_model.config.num_hidden_layers\n",
    "head_dim_orig = original_model.config.hidden_size // original_model.config.num_attention_heads\n",
    "kv_cache_size_original = 2 * 2 * seq_len * kv_heads * head_dim_orig  * n_attn_layers  # 2 for key and value, 2 for FP16\n",
    "print(f\"KV cache size: 2 * 2 * {seq_len}(seq_len) * {kv_heads}(kv_heads) * {head_dim_orig}(head_dim) * {n_attn_layers}(n_attn_layers) = {kv_cache_size_original / (1024 * 1024):.2f} MB\")\n",
    "print(f\"\\nKV cache size for {seq_len} tokens (FP16): {kv_cache_size_original / (1024 * 1024):.2f} MB\")\n",
    "print(f\"per token KV cache size: {kv_cache_size_original / (1024 * 1024 * seq_len):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Original Model\n",
    "\n",
    "Let's generate some text with the original model to compare it with the MLA version later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 17:13:35,977 - mla-demo - INFO - Generating text with original model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Model Output:\n",
      "Explain the advantages of Multi-head Latent Attention in language models in simple terms. I hope this helps!\n"
     ]
    }
   ],
   "source": [
    "# Set prompt\n",
    "prompt = \"Explain the advantages of Multi-head Latent Attention in language models in simple terms.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with original model\n",
    "logger.info(\"Generating text with original model...\")\n",
    "with torch.no_grad():\n",
    "    outputs_original = original_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text_original = tokenizer.decode(outputs_original[0], skip_special_tokens=True)\n",
    "print(f\"\\nOriginal Model Output:\\n{generated_text_original}\")\n",
    "\n",
    "# Free up memory\n",
    "del original_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 17:42:03,284 - mla-demo - INFO - Converting TinyLlama/TinyLlama-1.1B-Chat-v1.0 to MLA...\n",
      "2025-03-23 17:42:03,593 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-03-23 17:42:10,644 - root - WARNING - Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n",
      "2025-03-23 17:42:10,914 - mla_retrofit.convert - INFO - Original head dimension: 64\n",
      "2025-03-23 17:42:10,916 - mla_retrofit.convert - INFO - Original KV heads: 4\n",
      "2025-03-23 17:42:10,916 - mla_retrofit.convert - INFO - Target latent dimension: 256\n",
      "2025-03-23 17:42:10,918 - mla_retrofit.convert - INFO - Target KV heads: 4\n",
      "2025-03-23 17:42:10,920 - mla_retrofit.convert - INFO - Target head dimension: 64\n",
      "2025-03-23 17:42:10,921 - mla_retrofit.convert - INFO - Module model.layers.0.self_attn does not have MLA structure (missing k_up_proj or v_up_proj)\n",
      "2025-03-23 17:42:10,922 - mla_retrofit.convert - INFO - Model has MLA structure: False\n",
      "2025-03-23 17:42:10,923 - mla_retrofit.convert - INFO - Applying RoPE extend mode\n",
      "2025-03-23 17:42:10,925 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.0.self_attn\n",
      "2025-03-23 17:42:10,946 - mla_retrofit.convert - INFO - Reshaping model.layers.0.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:10,947 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:10,948 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:10,951 - mla_retrofit.convert - INFO - Reshaping model.layers.0.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:10,955 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.1.self_attn\n",
      "2025-03-23 17:42:10,963 - mla_retrofit.convert - INFO - Reshaping model.layers.1.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:10,964 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:10,964 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:10,966 - mla_retrofit.convert - INFO - Reshaping model.layers.1.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:10,971 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.2.self_attn\n",
      "2025-03-23 17:42:10,980 - mla_retrofit.convert - INFO - Reshaping model.layers.2.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:10,984 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:10,986 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:10,988 - mla_retrofit.convert - INFO - Reshaping model.layers.2.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:10,992 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.3.self_attn\n",
      "2025-03-23 17:42:11,002 - mla_retrofit.convert - INFO - Reshaping model.layers.3.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,005 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,009 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,012 - mla_retrofit.convert - INFO - Reshaping model.layers.3.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,016 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.4.self_attn\n",
      "2025-03-23 17:42:11,028 - mla_retrofit.convert - INFO - Reshaping model.layers.4.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,030 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,032 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,035 - mla_retrofit.convert - INFO - Reshaping model.layers.4.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,040 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.5.self_attn\n",
      "2025-03-23 17:42:11,051 - mla_retrofit.convert - INFO - Reshaping model.layers.5.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,054 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,054 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,054 - mla_retrofit.convert - INFO - Reshaping model.layers.5.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,063 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.6.self_attn\n",
      "2025-03-23 17:42:11,075 - mla_retrofit.convert - INFO - Reshaping model.layers.6.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,077 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,080 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,083 - mla_retrofit.convert - INFO - Reshaping model.layers.6.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,084 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.7.self_attn\n",
      "2025-03-23 17:42:11,099 - mla_retrofit.convert - INFO - Reshaping model.layers.7.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,103 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,105 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,107 - mla_retrofit.convert - INFO - Reshaping model.layers.7.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,109 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.8.self_attn\n",
      "2025-03-23 17:42:11,115 - mla_retrofit.convert - INFO - Reshaping model.layers.8.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,116 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,118 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,121 - mla_retrofit.convert - INFO - Reshaping model.layers.8.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,121 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.9.self_attn\n",
      "2025-03-23 17:42:11,130 - mla_retrofit.convert - INFO - Reshaping model.layers.9.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,131 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,133 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,138 - mla_retrofit.convert - INFO - Reshaping model.layers.9.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,139 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.10.self_attn\n",
      "2025-03-23 17:42:11,152 - mla_retrofit.convert - INFO - Reshaping model.layers.10.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,154 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,155 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,158 - mla_retrofit.convert - INFO - Reshaping model.layers.10.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,162 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.11.self_attn\n",
      "2025-03-23 17:42:11,165 - mla_retrofit.convert - INFO - Reshaping model.layers.11.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,173 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,174 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,176 - mla_retrofit.convert - INFO - Reshaping model.layers.11.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,178 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.12.self_attn\n",
      "2025-03-23 17:42:11,185 - mla_retrofit.convert - INFO - Reshaping model.layers.12.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,188 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,189 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,189 - mla_retrofit.convert - INFO - Reshaping model.layers.12.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,189 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.13.self_attn\n",
      "2025-03-23 17:42:11,201 - mla_retrofit.convert - INFO - Reshaping model.layers.13.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,203 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,205 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,205 - mla_retrofit.convert - INFO - Reshaping model.layers.13.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,209 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.14.self_attn\n",
      "2025-03-23 17:42:11,212 - mla_retrofit.convert - INFO - Reshaping model.layers.14.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,213 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,214 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,215 - mla_retrofit.convert - INFO - Reshaping model.layers.14.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,218 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.15.self_attn\n",
      "2025-03-23 17:42:11,223 - mla_retrofit.convert - INFO - Reshaping model.layers.15.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,225 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,226 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,228 - mla_retrofit.convert - INFO - Reshaping model.layers.15.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,231 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.16.self_attn\n",
      "2025-03-23 17:42:11,239 - mla_retrofit.convert - INFO - Reshaping model.layers.16.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,242 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,243 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,244 - mla_retrofit.convert - INFO - Reshaping model.layers.16.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,246 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.17.self_attn\n",
      "2025-03-23 17:42:11,253 - mla_retrofit.convert - INFO - Reshaping model.layers.17.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,254 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,255 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,257 - mla_retrofit.convert - INFO - Reshaping model.layers.17.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,260 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.18.self_attn\n",
      "2025-03-23 17:42:11,267 - mla_retrofit.convert - INFO - Reshaping model.layers.18.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,270 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,272 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,275 - mla_retrofit.convert - INFO - Reshaping model.layers.18.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,278 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.19.self_attn\n",
      "2025-03-23 17:42:11,284 - mla_retrofit.convert - INFO - Reshaping model.layers.19.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,286 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,287 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,289 - mla_retrofit.convert - INFO - Reshaping model.layers.19.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,292 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.20.self_attn\n",
      "2025-03-23 17:42:11,296 - mla_retrofit.convert - INFO - Reshaping model.layers.20.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,297 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,299 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,302 - mla_retrofit.convert - INFO - Reshaping model.layers.20.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,305 - mla_retrofit.convert - INFO - Creating k_up_proj and v_up_proj layers for model.layers.21.self_attn\n",
      "2025-03-23 17:42:11,313 - mla_retrofit.convert - INFO - Reshaping model.layers.21.self_attn.k_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,315 - mla_retrofit.convert - INFO - Target parameters: kv_heads=4, ori_head_dim=64, hidden_size=2048, latent_dim=256\n",
      "2025-03-23 17:42:11,315 - mla_retrofit.convert - INFO - Calculated group_size=1\n",
      "2025-03-23 17:42:11,318 - mla_retrofit.convert - INFO - Reshaping model.layers.21.self_attn.v_proj: Original shape torch.Size([256, 2048]), size 524288\n",
      "2025-03-23 17:42:11,338 - mla_retrofit.convert - INFO - Absorbing projections\n",
      "2025-03-23 17:42:11,338 - mla_retrofit.convert - INFO - Absorbing projections... This might take a while.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLA Model Configuration:\n",
      "Hidden size: 2048\n",
      "Number of attention heads: 32\n",
      "Number of KV heads: 4\n",
      "Head dimension: 64\n",
      "\n",
      "KV cache size for 1024 tokens (FP16): 1.00 MB\n",
      "Memory reduction: 95.45%\n"
     ]
    }
   ],
   "source": [
    "# Convert model to MLA\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mla_retrofit import convert_to_mla\n",
    "\n",
    "logger.info(f\"Converting {model_name} to MLA...\")\n",
    "\n",
    "model, tokenizer = convert_to_mla(\n",
    "    model_name_or_path=model_name,\n",
    "    num_kv_heads=num_kv_heads,\n",
    "    head_dim=head_dim,\n",
    "    rope_mode=rope_mode,\n",
    "    absorb=absorb,\n",
    "    flash_attn=False,  # Set to True if you have Flash Attention installed\n",
    "    return_model=True,\n",
    ")\n",
    "\n",
    "# Print MLA model configuration\n",
    "print(\"\\nMLA Model Configuration:\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"Number of KV heads: {model.config.num_key_value_heads}\")\n",
    "print(f\"Head dimension: {model.config.head_dim}\")\n",
    "\n",
    "# Calculate memory requirements\n",
    "kv_cache_size_mla = 2 * seq_len * num_kv_heads * head_dim * 2  # 2 for K and V, 2 bytes for float16\n",
    "print(f\"\\nKV cache size for {seq_len} tokens (FP16): {kv_cache_size_mla / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - kv_cache_size_mla / kv_cache_size_original) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with MLA Model\n",
    "\n",
    "Now let's generate text with the MLA-converted model and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with MLA model\n",
    "logger.info(\"Generating text with MLA model...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    logger.info(f\"Logits: {logits}\")\n",
    "\n",
    "    # Check for invalid values in logits\n",
    "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "        raise ValueError(\"Logits contain NaN or Inf values.\")\n",
    "\n",
    "    outputs_mla = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text_mla = tokenizer.decode(outputs_mla[0], skip_special_tokens=True)\n",
    "print(f\"\\nMLA Model Output:\\n{generated_text_mla}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Converted Model\n",
    "\n",
    "Let's save the MLA-converted model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "logger.info(f\"Saving MLA model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Longer Context\n",
    "\n",
    "One benefit of MLA is its ability to handle longer contexts with the same memory. Let's test this with a longer prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a longer prompt (repeated text for demo purposes)\n",
    "long_prompt = prompt + \"\\n\\n\" + \"\\n\\n\".join([f\"Section {i+1}: \" + prompt for i in range(10)])\n",
    "print(f\"Prompt length: {len(tokenizer.encode(long_prompt))} tokens\")\n",
    "\n",
    "# Tokenize long prompt\n",
    "long_inputs = tokenizer(long_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with MLA model on longer context\n",
    "logger.info(\"Generating text with MLA model on longer context...\")\n",
    "with torch.no_grad():\n",
    "    outputs_long = model.generate(\n",
    "        **long_inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text_long = tokenizer.decode(outputs_long[0], skip_special_tokens=True)\n",
    "# Just print the generated part (not the full prompt)\n",
    "print(f\"\\nMLA Model Output on Longer Context:\\n{generated_text_long[len(long_prompt):]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profile and Benchmark\n",
    "\n",
    "Now let's profile the memory usage and generation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory profiling\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCUDA Memory Stats:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved() / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    # Simple generation speed benchmark\n",
    "    import time\n",
    "    \n",
    "    # Warm-up\n",
    "    with torch.no_grad():\n",
    "        model.generate(**inputs, max_new_tokens=10)\n",
    "    \n",
    "    # Benchmark\n",
    "    num_runs = 5\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    print(f\"\\nBenchmarking generation speed (average of {num_runs} runs):\")\n",
    "    for i in range(num_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "        run_time = end_time - start_time\n",
    "        total_time += run_time\n",
    "        total_tokens += tokens_generated\n",
    "        \n",
    "        print(f\"Run {i+1}: Generated {tokens_generated} tokens in {run_time:.4f}s ({tokens_generated/run_time:.2f} tokens/s)\")\n",
    "    \n",
    "    print(f\"\\nAverage: {total_tokens/total_time:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the MLA Model (Optional)\n",
    "\n",
    "For even better performance, you may want to fine-tune the converted model. Here's a simplified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell is for demonstration only and won't run well in Colab without additional setup\n",
    "# Uncomment and run if you have sufficient GPU memory and want to try fine-tuning\n",
    "\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load a small dataset for fine-tuning (using alpaca as an example)\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:100]\")  # Just use 100 examples for demo\n",
    "\n",
    "# Format the dataset\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"### Instruction: {example['instruction']}\\n\\n### Input: {example['input']}\\n\\n### Response: {example['output']}\"\n",
    "    }\n",
    "\n",
    "# Apply formatting and tokenization\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"input\", \"output\", \"text\"],\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-mla-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_steps=10,  # Just a few steps for the demo\n",
    "    logging_steps=1,\n",
    "    save_steps=5,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./tinyllama-mla-finetuned\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Convert a model from standard attention or GQA to MLA\n",
    "2. Compare the memory usage before and after conversion\n",
    "3. Test the model on both short and longer context prompts\n",
    "4. Benchmark generation speed\n",
    "5. (Optionally) Fine-tune the converted model\n",
    "\n",
    "MLA-Retrofit provides a simple way to enhance existing models by adding the benefits of Multi-head Latent Attention without requiring full retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
